---
title: |
   Reproducibility in small-N treatment research: a tutorial through the lens of aphasiology
# Text within the ^ denotes superscript
author: "Robert Cavanaugh,^1,2^ Yina M. Quique,^3^ Alexander M. Swiderski,^1,2,4^ Lydia Kallhoff,^5^ Lauren Terhorst,^6^ Julie Wambaugh,^5^ William D. Hula,^2, 1^ William S. Evans^1^"
# There's no affiliation distinction in the word doc, so using abstract to fill the info 
# The actual abstract is included manually below. This also ensures abstract is on a separate page.
abstract: |
  1.	University of Pittsburgh, Department of Communication Sciences and Disorders
  2.	VA Pittsburgh Healthcare System, Audiology and Speech Pathology Program
  3.	Center for Education in Health Sciences, Northwestern University & Shirley Ryan Ability Lab
  4.	Carnegie Mellon University, Center for Neural Basis of Cognition
  5.	University of Utah, Department of Communication Sciences and Disorders
  6.	University of Pittsburgh, Department of Occupational Therapy
# specifies that the output will be a word document
output:
  word_document:
    # this holds the style template for the word document
    reference_docx: "templates-data/custom-reference.docx"
    # this makes the equation numbering work
    pandoc_args: ["-Fpandoc-crossref"]
# file with bibtex citations for the document. generated with the zotero plugin
# or using the {rbbt} R package
bibliography: "citations.bib"
# this is the apa style for the document
# More styles can be found here: https://www.zotero.org/styles
csl: apa.csl
---

<!-- Put your disclosures, keywords and abstract in this area between the two lines with :::. -->

<!-- <br> will create a blank line between text if needed.  -->

<!-- A \ at the end of a line will cause a linebreak.  -->

<!-- \newpage will create a page break to put the abstract on a new page -->

::: {custom-style="noIndentParagraph"}
<br>

The authors have no disclosures.\
Key words: aphasia treatment reproducibility replication

<br>

Corresponding Author:\
Robert Cavanaugh M.S. CCC-SLP\
rob.cavanaugh\@pitt.edu

\newpage

**Purpose**: Small-N studies are the dominant study design supporting evidence-based interventions in communication science and disorders, including treatments for aphasia and related disorders. However, there is little guidance for conducting reproducible analyses or selecting appropriate effect sizes in small-N studies, which has implications for scientific review, rigor, and replication. The purpose of this tutorial is to (1) demonstrate how to conduct reproducible analyses using effect sizes common to aphasia and related disorders and (2) provide a conceptual background to improve the reader's understanding of these effect sizes.

**Methods**: We provide a tutorial on reproducible analyses of small-N designs in the statistical programming language R using published data from Wambaugh et al. (2017). Additionally, we discuss the strengths, weaknesses, reporting requirements, and impact of experimental design decisions on effect sizes common to this body of research.

**Results**: Reproducible code demonstrates implementation and comparison of within-case standardized mean difference, proportion of maximal gain, Tau-U, and frequentist and Bayesian mixed-effects models. Data and code are available as a resource for researchers, clinicians, and students.

**Conclusion**: Pursuing reproducible research via sharing data and analytical scripts is key to promoting transparency in small-N treatment research. Researchers and clinicians need to understand the properties of common effect size measures to make informed decisions when selecting effect sizes and act as informed consumers of small-N studies. Together, a commitment to reproducibility and a keen understanding of effect sizes can improve the scientific rigor and synthesis of the evidence supporting clinical services in communication sciences and disorders.
:::

\newpage

```{r, include = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set()
bayesian_es = readRDS(here::here("manuscript", "templates-data", "bayesian_es.rds"))
```

Researchers in communication sciences and disorders (CSD) make many choices in the design, conduct, and reporting of their research. These "researcher degrees of freedom" have the potential to increase the frequency of false-positive findings, inflate effect sizes, and impair successful replication [@simmons2011false]. Study pre-registration, sharing of data and reproducible workflows, and replication efforts are potential solutions for methodically integrating these researcher degrees of freedom within the scientific endeavor. However, sharing of reproducible analytical code and data, which promotes transparency in scientific decision-making, remains scarce in CSD, as acknowledged by this special issue. Ultimately, a lack of transparency negatively impacts the scientific review, replication, synthesis, and real-world impact of research in CSD.

One area where this lack of transparency may have a large impact is in small-N treatment studies in aphasia and related disorders. Small-N studies, including experimental and non-experimental single-case designs and within-subject case series designs, are the "dominant" intervention design in neurogenic communication disorders and aphasiology, comprising the bulk of interventional evidence base [@murray2013reliability; @togher2009methodological]. Small-N studies typically focus on treatment response at the individual level and establish experimental control within each participant rather than via a control group [@thompsonEstablishingEffectsTreatment2015]. While single-case experimental studies typically include at least 2-4 participants, these designs are often extended to within-subject case-series designs with upwards of thirty participants [e.g., @gilmoreTypicalitybasedSemanticTreatment2020], useful for testing psycholinguistic theories and exploring individual differences in treatment response [@nickelsChallengesUseTreatment2015a].

In contrast to group-level studies (e.g., randomized controlled trials), small-N designs confer advantages such as reduced cost, lower recruitment demands [@kratochwillSinglecaseInterventionResearch2014], and the ability to evaluate patterns of treatment response at the individual level. They can offer a cost-effective means of piloting novel interventions as a precursor to large-scale trials while minimizing concerns related to statistical power. Insights into individual-level responses to treatment are also crucial for studying heterogeneous populations and for clinical providers who provide intervention at the individual level [@portneyFoundationsClinicalResearch2015].

However, statistical analysis of small-N studies in aphasia and related disorders varies widely, and there is little guidance for the selection, implementation, and reporting of reproducible analyses in small-N studies. Effect sizes, which characterize the magnitude of treatment response, are a particular source of consternation and disagreement [@howardOptimisingDesignIntervention2015]. The choices researchers make in selecting and implementing effect size measures can negatively affect study replication. These choices are often insufficiently reported or acknowledged, such that their impact on study reproducibility and replication is not apparent.

To address these challenges, this tutorial has two aims: (1) To demonstrate how to conduct reproducible analysis using effect sizes common to small-N treatment research on aphasia and related disorders using the statistical programming language R [@rcoreteamLanguageEnvironmentStatistical2020] and (2) To discuss how the selection and implementation of effect sizes can affect the interpretation, replication, and synthesis of the literature. It will do so by re-analyzing published data from a recent series of multiple-baseline single-case experimental design studies on sound production treatment for post-stroke apraxia of speech [@wambaugh2014sound; @wambaugh2016sound; @wambaugh2017effects].

This work is not intended to provide a comprehensive tutorial on small-N experimental design methodology, statistical programming, or the analytical methods within (e.g., mixed-effects models or Bayesian statistics). For each of these, we provide recommendations for further reading throughout the paper. Instead, our intent is that this paper will serve as a practical starting place for researchers engaged in small-N studies to begin incorporating reproducible analyses into their regular workflow. Moreover, we hope that researchers are able to make more informed choices about effect sizes in their studies and that researchers and clinicians can be more informed consumers of analytical approaches in small-N designs.

## Reproducibility in small-N designs in aphasia and related disorders

Reproducibility is typically defined as consistently producing the same results from the raw data gathered in a study [@nosek2017reproducibility]. In the small-N design context, reproducibility requires a well-documented processing stream that begins with individual session-level data collected at each probe, and ends with finalized figures demonstrating performance over time and statistical results reporting the certainty and magnitude of change. In small-N studies, this processing stream often includes manual entry of individual probe data into spreadsheets, manipulations of data within spreadsheets, and manual creation of figures. These types of processing streams risk failures in process reproducibility, where the original analysis cannot be repeated because of underspecified or missing procedural details necessary to reproduce the analysis [@nosek2022replicability]. Even in cases where the analytical process is well-documented, human-mediated procedures leave room for errors [@strand2021error], risking failures of outcome reproducibility - when a reanalysis obtains a different result than originally reported [@nosek2022replicability].

Script-based analyses using statistical programming languages are one solution for improving reproducibility in small-N designs [@hardwicke2018data; @kidwell2016badges]. Using analysis scripts allow researchers to document each step of the processing stream while peers and collaborators can validate the analytical pipeline as part of the research workflow. When data and scripts are shared, external researchers can reproduce study results and more easily replicate the analysis in future replication studies. However, script-based analyses and statistical programming may be unfamiliar to researchers in CSD, and may pose a high barrier to entry. Therefore, the first aim of this tutorial is to demonstrate how to use the statistical programming language R to calculate effect sizes in a reproducible workflow using published data from Wambaugh and colleagues (2017).

## Effect sizes in small-N designs in Aphasiology

For small-N studies in CSD, characterizing the response to treatment is central to understanding intervention efficacy, candidacy, and the theoretical mechanisms that underlie treatment success [@kratochwillSinglecaseInterventionResearch2014]. However, the optimal methodology for measuring treatment response in small-N studies remains an area of disagreement [@howardOptimisingDesignIntervention2015]. In general, clinical researchers seek to establish 1) whether or not a treatment effect exists (i.e., statistical significance testing) and 2) an effect size that characterizes the magnitude of treatment response. Effect sizes are essential for validating the clinical relevance of interventions, where the magnitude of treatment response is arguably at least as important as its statistical significance. Greater evidence for an intervention can be established by meta-analysis of multiple small-N studies, which typically focus on synthesizing effect sizes. Within-subject case-series experimental designs often rely on precise estimates of individual effect sizes to evaluate relationships between individual factors and treatment response [@rappCaseSeriesCognitive2011]. Precise effect sizes are also important for estimating statistical power in subsequent trials.

There are also domain-specific considerations for estimating effect sizes in small-N designs in aphasia and related disorders. Heterogeneity in language abilities and performance variability is inherent to the nature of aphasia, and interventions often engender a wide range of treatment responses. Trends during the baseline phase are common, making it difficult for researchers to differentiate treatment response from repeated testing effects. Finally, the outcome variables of interest in most small-N studies in aphasiology are often generated from closed sets of stimuli or treatment targets, which benefit from careful modeling to appropriately characterize effect sizes and promote the generalizability of study findings [@wileyStatisticalAnalysisSmallN2018].

However, effect sizes used in small-N studies are often sensitive to experimental design choices, which can obscure successful conceptual replication - our ability to support the same hypothesis through different experimental approaches. Effect sizes may be sensitive to experimental elements such as the difficulty, nature, or number of experimental stimuli, how stimuli are matched to participants characteristics, or the number of observations in the baseline and treatment phases. As a result, even when investigators take steps to ensure that workflows are reproducible, the choice of effect size can influence interpretation and replicability within and across small-N studies. This challenge motivates the second purpose of the present study, which is to help researchers and clinicians make informed decisions and be informed consumers of effect sizes common to small-N research.

In the following sections, we will review the conceptual definitions of effect sizes common to the small-N research in aphasia and related disorders and demonstrate their implementation in R using data from @wambaugh2017effects. Afterward, we will calculate and compare each effect size for all cases in Wambaugh et al. to motivate a discussion of the strengths and limitations of each measure.

# Case example: reproducible re-analysis of Wambaugh et al., 2017

@wambaugh2017effects reported the effects of sound production treatment for apraxia of speech and aphasia for 20 individuals in a multiple-baseline single-case experimental design under two experimental conditions: blocked and random practice. Sound production treatment uses "therapeutic techniques of modeling and repetition, contrastive practice, orthographic cueing, integral stimulation, and articulatory cueing" in a "response-contingent hierarchy" to target phoneme production (p. 1744). The expectation is that repeated practice based on principles of motor learning will improve the production of target phonemes in the treated context, and (ideally) generalize to the production of those phonemes in words that are not explicitly treated.

In Wambaugh et al., (2017), participants received treatment on two lists, one in each experimental condition (random and blocked). Each list contained treated items and untreated generalization items for two target phonemes. Items consisted of a single word or occasionally a 2-3 word phrase. For 16/20 participants, treated lists contained 20 items (10 for each phoneme), and untreated lists contained 10 items (5 for each phoneme). For these participants, accuracy was determined based on the production of the target phoneme within the item. For 4/20 participants, treated lists contained 10 items (5 for each phoneme), and untreated lists contained 6 items (3 for each phoneme). For these participants, accuracy was determined based on the production of the entire item (see Wambaugh et al., 2016 for details on these four participants). While Wambaugh et al., (2017) aimed to compare the effects of randomized versus blocked practice on treatment outcomes, this tutorial will focus on calculating effect sizes.

A reproducible analysis of data from Wambaugh et al., (2017) in R begins by loading necessary packages and setting a seed for reproducibility. The data are stored in separate probe files for each participant and session, as small-N data are typically collected. By programmatically reading and combining raw probe data from each session, we avoid modifying the data by hand and minimize the chance for errors when combining the data manually.

```{r, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)      # data wrangling
library(SingleCaseES)   # calculating SMD, Tau-U
library(lme4)           # frequentist mixed-effects models
library(emmeans)        # estimating effect sizes from lme4
library(brms)           # bayesian mixed-effects models
library(tidybayes)      # estimating effect sizes from brms
library(here)           # for locating files

set.seed(42)            # set a seed for reproducibility

# create a list of files
files <- list.files(
                 here("study-data"), # look in the study-data folder
                 full.names = TRUE,  # use the full paths of the files
                 pattern = ".csv",   # only read in .csv files
                 recursive = TRUE)   # include files within subfolders

# read in the files and combine them together
# save the resulting dataframe in an abject called “df”
df <- files %>%
  map_dfr(read_csv, show_col_types = FALSE)
```

The data is organized in a "tidy" format where each column represents a variable and each row represents an observation, or a single response to a single item [@wickhamTidyData2014]. See Table 1. for a codebook for column names and Table 2. for the first 5 rows of the data.

##### Table 1. About here

```{r, echo = FALSE, include = FALSE, warning = FALSE, message = FALSE}
library(flextable)
library(officer)
library(insight)
# If this is a preprint with preprint formatting, include this chunk
# otherwise don't include the chunk. JSLHR requires tables in a word doc
# or excel file rather that in the manuscript file. 
# This code reads in a template, generates table 2, and then saves it as
# a JSLHR compliant formatted word doc
cols = tibble(
  Variable = colnames(df),
  Description = c(
    "de-identified participant ID",
    "probe schedule (blocked or random)",
    "target_phoneme",
    "item condition (treatment or generalization)",
    "treatment phase",
    "session number from Wambaugh 2017",
    "item identifier",
    "number of items in the list (per phoneme)",
    "phase used to calcualte effect sizes in Wambaugh et al., 2017",
    "accuracy of participant response",
    "Number of baseline sessions"
  )
) 

t1 <- flextable(cols, cwidth = c(2,3))
doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t1)
fileout <- here("manuscript", "tables-figures", "table1.docx") # uncomment to write in your working directory
print(doc, target = fileout)

# print the table. this will only work if include is TRUE
cols %>%
    insight::print_md(
      caption = "Table 1. Variables and descriptions for study data from Wambaugh et al., (2017)",
      align = "ll")
```

<!-- <br> -->

##### Table 2. About here

```{r, echo = FALSE, include = FALSE, warning = FALSE, message = FALSE}
# If this is a preprint with preprint formatting, include this chunk
# otherwise don't include the chunk. JSLHR requires tables in a word doc
# or excel file rather that in the manuscript file. 
# This code reads in a template, generates table 2, and then saves it as
# a JSLHR compliant formatted word doc

# create a flextable object using the first five rows of data
t2 <- flextable(head(df, 5), cwidth = 0.9)

# read in a blank template with the correct formatting
doc <- read_docx(here("manuscript", "templates-data", "template_lnd.docx"))
# add the table to the document
doc <- body_add_flextable(doc, value = t2)
# filename within the path we want
fileout <- here("manuscript", "tables-figures", "table2.docx") # uncomment to write in your working directory
print(doc, target = fileout)

head(df, 5) %>% insight::print_md(
  caption = "Table 1. The first 5 rows of data"
)
```

<!-- <br> -->

In the following sections, we demonstrate how to calculate each effect size for a single case participant (participant 10, blocked condition) from Wambaugh et al., (2017). P10's performance on the blocked condition is shown in Figure 1. R code for calculating effect sizes for all participants, item-types, and conditions in the study is available in the supplemental material S2. We can subset the data for participant 10's data as follows.

```{r, message = FALSE, warning = FALSE, cache = TRUE}
# Create a dataframe holding only data for participant 10
# The new dataframe is stored in an objected called “P10”
P10 <- df %>%
  # filter for participant 10, treated condition, blocked condition
  filter(participant == "P10",
         itemType == "tx",
         condition == "blocked") %>%
  # calculate the sum for each level of session, phase and spt2017
  group_by(session, phase, spt2017) %>%
  summarize(sum_correct = sum(response), .groups = "drop")
```

##### Figure 1. about here

```{r, fig.width = 5, fig.height = 3, warning = FALSE, include = FALSE}
p1 = P10 %>%
  # filter for baseline and treatment phases
  filter(phase == "baseline" | phase == "treatment") %>%
  # create a new variable called Measure that has a value of
  # include if spt2017 is not an NA value and exclude if it is. 
  # the levels argument indicates that exclude should be
  # the first level of the factor.
  mutate(Measure = factor(
    ifelse(!is.na(spt2017), "include", "exclude"),
           levels = c("exclude", "include"))) %>%
  # create a plot with session on the x axis, percent 
  # correct on the y axis, and group by phase
  ggplot(aes(x = session, y = sum_correct/20, group = phase)) +
  # add points to the graph
  geom_point(aes(alpha = Measure), size = 3) + 
  # add a line to the graph
  geom_line(alpha = 0.5) +
  # add a vertical line where x = 7
  geom_vline(aes(xintercept = 7), linetype = "dashed") +
  scale_alpha_discrete(range = c(0.35, 0.9)) +
  labs(title = "Participant 10, treated words, blocked condition",
       caption = "Dark circles represent data points used to calculate
       the within-case standardized mean difference in Wambaugh et al.,
       (2017)",
       y="Percent Correct") +
  guides(alpha = "none") +
  theme_grey(base_size = 12) + 
  labs(caption = NULL,
       x = "Probe Session Number",
       title = NULL) + 
  annotate("text", x=1, y=.975, label = "baseline", size = 4, hjust =.3, fontface = "italic") +
  annotate("text", x=8, y=.975, label = "treatment", size = 4, hjust =0.2, fontface = "italic") +
  scale_x_continuous(breaks = seq(0,26, 2)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1), limits = c(0,1))

ggsave(p1, filename = here("manuscript", "tables-figures", "p1.png"), bg = "white", width = 8, height = 5, dpi = 400)

p1
```

## Within-case Standardized Mean Difference

Beeson and Robey [-@beesonEvaluatingSinglesubjectTreatment2006] advocated for using a within-case standardized mean difference in single-case subject designs and meta-analyses of aphasia single subject research. It is one of, if not the most commonly used measure in the field [@antonucciAphasiaCoreOutcome2019]. The within-case standardized mean difference was initially proposed by Gingerich [-@gingerichMetaanalysisAppliedTimeseries1984] and later, Busk & Serlin [-@buskMetaanalysisSinglecaseResearch1992] as an individual-level effect size measurement that could be synthesized in meta-analysis. It was originally defined as the difference in means between the treatment and baseline phase divided by the standard deviation of the baseline phase (Equation 1). Beeson and Robey (2006) modified the measure, subtracting the mean of the baseline phase from a post-treatment phase, which they argued would better capture full treatment effects (henceforth, $d_{BR}$). This within-case $d_{BR}$ statistic represents the mean change between the end of the treatment phase and the baseline phase divided by the amount of variability during the baseline phase. It relies on the assumption that observations (i.e., probe sessions) are mutually independent, and that variability is present and constant within the baseline phase. $d_{BR}$ is unbounded, with values greater than zero indicating a positive response to treatment. When there is no variability in the baseline phase, researchers must decide whether to pool the standard deviation across phases, use the baseline variability from another condition or participant, or omit the measure for that series. It is typically interpreted based on established benchmarks in terms of "small," "medium," and "large".

$$d_{BR} = \frac{x_B - x_A}{S_A}$${#eq:eqn1}

To calculate $d_{BR}$ for a single set of data in R, we can calculate the mean of the baseline scores ($x_A$) , the mean of the treatment scores ($x_B$), and the standard deviation of the baseline scores ($S_A$). The $d_{BR}$ statistic is then calculated by subtracting the baseline scores from the treatment scores and dividing by the standard deviation. In Wambaugh et al., (2017) the $d_{BR}$ statistic was calculated using the last five baseline timepoints leading up to treatment and the last two timepoints in the treatment phase. This information is already included in the dataset (column "spt2017"), which has values "pre" for baseline, and "post" for treatment for timepoints that were included in this calculation, and NA for values not included in the calculation.

While it is relatively straightforward to write a function to calculate $d_{BR}$ in R, here we will use a function from a published package, SingleCaseES, which includes a variety of effect size methods common to single-case experimental designs [@pustejovsky2018singlecasees]. We can use the `SMD()` function from the SingleCaseES package to calculate $d_{BR}$ for our single case of participant 10. We start by extracting the number of correct items in the A and B phases, for probes used by Wambaugh et al., (2017) to calculate $d_{BR}$. Those data are then used to calculate $d_{BR}$.

```{r, warning = FALSE, message = FALSE, cache = TRUE}
# Extract the outcomes for the pre- and post-treatment
# Save the outcomes in two objects, called “A” and “B”
A = P10 %>% filter(spt2017 == "pre") %>% pull(sum_correct)
B = P10 %>% filter(spt2017 == "post") %>% pull(sum_correct)
# Calculate d BR using the two vectors above
SMD(A_data = A, B_data = B)$Est
```

Note that while the SMD() function also returns a confidence interval, its not clear how appropriate this confidence interval for $d_{BR}$, as the standard error is based on the original within-case standardized mean difference.

## Proportion of Potential Maximal Gain.

Lambon Ralph and colleagues [-@lambonralphPredictingOutcomeAnomia2010] proposed the proportion of potential maximal gain (PMG) as a method for describing the relative magnitude of improvement, accounting for baseline performance. PMG was intended to be used in analyses where participants received a different number of treated items or to account for baseline severity when the same items were assigned to all participants [@lazarImprovementAphasiaScores2010; @snellHowManyWords2010]. This feature makes PMG particularly relevant to the present dataset, where a fifth of participants received a modified SPT with fewer treated and untreated items.

PMG is defined as the difference in the average number of correct responses between the end of treatment and the baseline phase divided by the number of items available to gain during the treatment phase (i.e., the number of items treated less the average number correct during the baseline phase; Equation 2). PMG ranges between -1 and 1, where values near zero indicate no change and positive values indicate improvement. PMG can be interpreted as the proportion of improvement relative to the amount of possible improvement after the baseline phase.

$$PMG = \frac{x_B- x_A}{n_{Items} - x_A}$${#eq:eqn2}

There is no R package that includes a function to calculate PMG to our knowledge. However, creating such a function is relatively straightforward. A function that calculates PMG similar to the SMD() function from the SingleCaseES package might take the following form, first calculating the mean of the A phase and B phase, and then calculating the PMG statistic.

```{r, warning = FALSE, message = FALSE, cache = TRUE}
# A function for calculating PMG that takes 3 arguments:
# Vectors of the pre-treatment data and the post-treatment data 
# and the number of treated items

PMG <- function(a_data, b_data, nitems){
  mean_a <- mean(a_data) # calculate mean of the pre-treatment data
  mean_b <- mean(b_data) # calculate mean of post-treatment data
  change_score <- mean_b-mean_a
  potential_gain <- nitems-mean_a
  pmg <- change_score / potential_gain # calculate PMG
  return(pmg)
}

# Use the new function with the A and B data from above
PMG(a_data = A, b_data = B, nitems = 20)
```

## Tau-U

Tau-U was proposed by Parker et al. [-@parkerCombiningNonoverlapTrend2011] as a collection of non-parametric effect size measures that use Kendall's Rank Correlation to evaluate the independence of performance between study phases. Unlike other approaches discussed in this article, Tau-U is intended to evaluate the degree of non-overlap between treatment phases rather than the total magnitude of change between phases (Parker et al., 2011). The Tau statistics are essentially a rescaling of non-overlap of all pairs [@tarlowImprovedRankCorrelation2017] to the range [-1, 1], where 0 indicates no change and positive values indicate increasing independence between study phases. In aphasia and related disorders, Tau-U has generally referred to the case of Tau-U with a correction for baseline trends ($\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$). The baseline correction is typically applied if the baseline slope exceeds a cut-off ideally set a-priori, depending on the researcher's preference [@leeTauUQuantitativeApproach2018]. To be consistent with the aphasia literature, we will refer to Tau-U as the general statistic, specifying $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$ or $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$ where relevant.

We can calculate Tau-U as outlined by Lee and Cherney (2018) in R using SingleCaseES First, the `lm()` function (linear regression) is used to calculate the slope of any baseline trend.

```{r, message = FALSE, warning = FALSE, cache = TRUE}
# start with the dataframe for participant 10
P10 %>% 
    # filter for only baseline observations
    filter(phase == "baseline") %>%
    # run a linear regression to calculate the slope of performance
    lm(data = ., sum_correct~session) %>%
    # extract the coefficients of the regression
    coef()
```

Using the conservative benchmark of 0.33 recommended by Lee and Cherney (2018), we would calculate $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$ (without a baseline trend correction), as the slope of the baseline phase is only 0.2. To calculate $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$, we can use the Tau() function. Note that for Tau-U, we use all observations in the baseline and treatment phases.

```{r, message = FALSE, warning = FALSE, cache = TRUE}
# Extract the outcomes for the baseline and treatment phases
# Save the outcomes in two objects, called “A” and “B”
A = P10 %>% filter(phase == "baseline") %>% pull(sum_correct)
B = P10 %>% filter(phase == "treatment") %>% pull(sum_correct)
# Calculate Tau-U without trend correction using the two vectors above
Tau(A_data = A, B_data = B)
```

However, if we had elected to correct for baseline trends and use $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$, we can use the `Tau_U()` function.

```{r, message = FALSE, warning = FALSE, cache = TRUE}
# Calculate Tau-U with trend correction using the two vectors above
Tau_U(A_data = A, B_data = B)
```

In this case, $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$ is 1, as there are no treatment observations equal to or less than any one baseline observation, but $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$ is 0.95 due to a small baseline trend correction.

## Generalized Linear Mixed-effects Models

Linear mixed-effects models (also hierarchical models, multilevel models) have grown in popularity over the past decade and confer several advantages over traditional repeated measures analyses. Such advantages include the ability to analyze trial-level responses (e.g., correct, incorrect) rather than overall session accuracy [@jaegerCategoricalDataAnalysis2008], accommodate unbalanced designs and missing outcome data, and account for variation in both participants and stimuli simultaneously, thereby producing more generalizable findings [@baayenMixedeffectsModelingCrossed2008]. In small-N designs, mixed-effects models can adjust for baseline trends in performance, and allow researchers to evaluate interactions between treatment effects and other variables such as treatment condition or disorder severity, and can characterize non-linear changes in performance. Finally, the generalization of mixed-effects models beyond the linear case (generalized linear mixed-effects model; GLMM) permits researchers to appropriately characterize the dependent variable using a more appropriate probability distribution and link function, e.g., a binomial distribution and logistic link for x successes in n trials, or a poisson distribution and log link function to analyze count data. We direct readers to Wiley and Rapp [-@wileyStatisticalAnalysisSmallN2018] for a primer on mixed-effects models in aphasia research and multiple recent tutorials focused on research in communication sciences and disorders [@gordonHowMixedeffectsModeling2019; @harelMultilevelModelsCommunication2019].

There are a number of different approaches to both modeling longitudinal data for one or more participants in single-case and small-N data designs using mixed-effects models. In this paper, we will review an approach we have previously used in small-N designs in aphasiology [@evansPlayingBEARSBalancing2021; @swiderski2021treatment] that we find to align well with our conceptual model of multiple baseline designs, noting that the general concepts likely apply to similarly structured models. This interrupted time series model was originally advocated for by Huitema & McKean [-@huitemaDesignSpecificationIssues2000c] in the form of a standard linear model (equation 3).

$$Y_{t} = \beta_{0} + \beta_{1}T_{t} + \beta_{2}D_{t} + \beta_{3}[T_{t}-(n_{1}+1)]D_{t} + \epsilon _{t}$${#eq:eqn3}

The model describes the outcome *Y* at time *t* using fixed parameters for a baseline slope ($\beta_{1}$), level change immediately following the onset of treatment ($\beta_{2}$, and slope change $\beta_{3}$), representing the trend difference between the baseline and treatment phases. In this case, T represents the probe session number at time *t*, and n~1~ represents the number of baseline sessions [@Huitema2011-dv]. The level change and slope change parameters can be interpreted as effect sizes for their respective components. Additionally, an overall effect size and 95% confidence interval can be obtained by examining the estimated difference in performance between the end of baseline and end of treatment, accounting for a baseline trend if desired.

We can extend this model to a generalized linear mixed-effects model for participant 10, by modeling the response for each item (correct or incorrect) using a binomial distribution and logistic link function. The model's fixed effects [the primary effects of interest; @searleVarianceComponents1992] include baseline slope, level change, and slope change. The random effects (effects for which there is interest in generalizing to the underlying population e.g., all potential treated items) allow each item to have its own intercept and slope (see S1 for the model equation). This is considered a two-level model as repeated measures (items) are nested within observations (probe sessions) for a single participant. To implement the two-level model, we start by filtering the original data for the same blocked condition for participant 10, but maintain the item-level data. Then, we create the level change and slope change parameters according to equation 3. The `baseline_slope` variable is equivalent to the probe session number. The `level_change` variable is a categorical dummy variable with values 0 for baseline, and 1 for treatment. The `slope_change` variable multiplies the number of baseline sessions (6) plus 2 by level change; this value is then subtracted from the `baseline_slope` variable. Model coefficients are visualized in Figure 2.

```{r, warning = FALSE, message = FALSE, cache = TRUE}
# Create a dataframe holding item-level data for participant 10
# The new dataframe is stored in an objected called “P10”
P10 <- df %>%
  # filter for P10, blocked condition, treated items, 
  # baseline or treatment phases
  filter(participant == "P10",
         condition == "blocked",
         itemType == "tx",
         phase == "baseline" | phase == "treatment") %>%
  # baseline slope is equivalent to the session variable
  # level change is 0 for the baseline phase and 1 for treatment phase
  # slope change is calculated by subtracting the total number of baseline
  # sessions + 2 from the baseline slope variable, and multiplying the
  # result by the level change variable. The level change variable is
  # then converted to a factor (categorical) variable.
  mutate(baseline_slope = session,
         level_change = ifelse(phase == "baseline", 0, 1),
         slope_change = (baseline_slope - (6+2))*level_change,
         level_change = as.factor(level_change))
```

##### Figure 2. about here

```{r, echo = FALSE, fig.width = 5, fig.height = 3, include = FALSE}

P10_itts = df %>%
  filter(participant == "P10",
         itemType == "tx",
         condition == "blocked") %>%
  group_by(session, phase, spt2017) %>%
  summarize(sum_correct = sum(response), .groups = "drop")

pred_dat = P10_itts %>% filter(phase == "baseline" | phase == "treatment") %>%
  mutate(phase = ifelse(phase == "baseline", 0, 1),
         slope_change = (session-8)*phase)

# regular old glm model aggregated binomial
mod = glm(cbind(sum_correct, 20-sum_correct) ~ session + phase + slope_change, 
          family = binomial,
          data = pred_dat)

# the fitted line from the model on the response scale (percept)
pred_dat$preds = predict(mod, type = "response")

# make another dataframe, this time with no slope change to show
# a dashed line that has the baseline slope and level change
# incorporated but no slope change
new_dat = pred_dat %>%
  mutate(slope_change = 0)
new_dat$preds = predict(mod, newdata = new_dat, type = "response")

# make another dataframe, this time with no slope change or level change
# to show the continuing baseline slope to the point of level change
new_dat2 = pred_dat %>%
  mutate(phase = 0, slope_change =0)

new_dat2$preds = predict(mod, newdata = new_dat2, type = "response")

# only show it for sessions before 9
new_dat2 = new_dat2 %>%
  filter(session<9)

# here's the plot
# I commented out the "ggbrace" line that creates the level change
# brace and label so you don't
# have to install that additional package. If you want to add those, 
# see here: https://github.com/NicolasH2/ggbrace and then 
# uncomment the line starting with ggbrace::
p2 = pred_dat %>%
  ggplot(aes(x = session, y = sum_correct/20, group = phase)) +
  geom_point(size = 4) + 
  geom_line(alpha = 0.25) +
  geom_vline(aes(xintercept = 7), linetype = "dashed") +
  labs(title = "Participant 10, treated words, blocked condition",
       y="Percent Correct") +
  guides(alpha = "none") +
  # fitted line
  geom_line(inherit.aes = FALSE, data = pred_dat,
            aes(x=session, y = preds, group = phase), color = "darkred") + 
  # baseline slope plus level change line
  geom_line(inherit.aes = FALSE, data = new_dat,
            aes(x=session, y = preds, group = phase), color = "darkred", alpha = 0.3, linetype = "dashed")+
  # extended baseline slope dashed line
  geom_line(inherit.aes = FALSE, data = new_dat2,
            aes(x=session, y = preds, group = phase), color = "darkred", alpha = 0.3, linetype = "dashed")+
  ggbrace::geom_brace(aes(c(8,9), c(0.19, 0.5),
   label = "level change"), inherit.data=F, rotate = 90, labelsize = 4, color = "darkred") +
  annotate(color = "darkred",
           geom = "curve", x = 4, y = .35, xend = 2, yend = .15, 
           curvature = .3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = 1, y = .375, label = "baseline slope",
           hjust = "left", color = "darkred") +
  annotate(color = "darkred",
           geom = "curve", x = 4, y = .35, xend = 2, yend = .15, 
           curvature = .3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = 1, y = .375, label = "baseline slope",
           hjust = "left", color = "darkred") +
  annotate(color = "darkred",
           geom = "curve", x = 15, y = .65, xend = 15, yend = .76, 
           curvature = .3, arrow = arrow(length = unit(2, "mm"), ends = "both")
  ) +
  annotate(color = "darkred",
           geom = "text", x = 15.75, y = .77, hjust = "left",
           label = "slope change"
  ) + 
  labs(caption = NULL,
       x = "Probe Session Number",
       title = NULL) + 
  annotate("text", x=1, y=.975, label = "baseline", size = 4, hjust =.3, fontface = "italic") +
  annotate("text", x=8, y=.975, label = "treatment", size = 4, hjust =0.2, fontface = "italic") +
  scale_x_continuous(breaks = seq(0,26, 2)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1), limits = c(0,1))

ggsave(p2, filename = here("manuscript", "tables-figures", "p2.png"), bg = "white", width = 8, height = 5, dpi = 400)

p2
```

In the code below, the model is assigned to the object `mod1`. `response` is the dependent variable, consisting of a 1 for correct and 0 for incorrect responses. The three fixed effects include baseline slope, level change, and slope change. The model also includes random intercepts for each item and random slopes for baseline slope, level change, and slope change. The family argument indicates that the dependent variable follows a binomial distribution. An additional argument specifying an optimizer is included to improve model convergence.

```{r, warning TRUE, message = FALSE, cache = TRUE}
# The resulting model is saved as an object called “mod1”
mod1 <- 
    # a mixed-effects model starts with the glmer() function
    glmer(
         # response is the dependent variable (0 or 1)
         # The independent variables come after the “~” symbol
         # the fixed effects 
         response ~ baseline_slope + level_change + slope_change +
         # random effects are in parentheses. item is a random intercept
         # while the three effects before the “|” symbol are random slopes
         (1 + baseline_slope + level_change + slope_change | item),
         # specify the data
          data = P10,
         # specify the distribution of the dependent variable
          family = binomial,
         # the optimizer is intended to help with convergence
          control = glmerControl(optimizer = "bobyqa"))
```

Finally, we can extract the model summary using the `summary()` command. The fixed effects portion of the model summary is reported below.

```{r, warning = FALSE, message = FALSE, cache = TRUE}
# remove $coefficients to display the entire summary
summary(mod1)$coefficients
```

Model coefficients are returned in logits (or log-odds). Briefly, these results indicate that performance immediately prior to baseline is predicted to be about `r round(plogis(fixef(mod1)[[1]])*100, 1)`% (achieved by converting the intercept log-odds to probability). The odds of a correct response rose marginally during the baseline phase, was `r round(exp(fixef(mod1)[[3]]), 1)` times greater after the first treatment session (calculated by exponentiating the level change log-odds value of `r round(fixef(mod1)[[3]], 1)`), and increased by a rate that was `r round(exp(fixef(mod1)[[4]]), 1)` times greater during treatment than it was during baseline. Neither the level change nor the slope change coefficient was statistically significant at an alpha level of .05.

<!-- Note that following values come from S1, and it was easier just to hand write them here, unfortunately -->

An overall effect size can be calculated using the emmeans package [@lenthEmmeansEstimatedMarginal2021] by contrasting the estimated model performance at the last treatment session, with and without the level change and slope change parameters (see supplemental materials S1). This approach assumes that the baseline trend would have continued linearly in the absence of treatment and indicates that the odds of a correct response were 6.8 times greater as a result of treatment at the last treatment session (p = 0.16). Alternatively, in cases where the baseline phase is stable before the onset of treatment, we can contrast performance at the end of treatment from performance at the end of the baseline. This approach indicates that the probability of a correct response was 9.4 times greater at the end of treatment compared to the end of baseline (p = .0007).

When there are multiple participants in a study, this approach can be extended to a three-level model with random effects for both participants and items. Because a three-level model can permit participants to vary in their intercept (performance at the start of the study), baseline slope, level change, and slope change, three-level models can characterize change both on average and for each participant. However, one limitation of the frequentist implementation of the three-level model is that it is difficult to estimate confidence intervals for individual participants.

## Bayesian Mixed-effects Models

Bayesian implementations of mixed-effects models often resolve some challenges to implementing mixed-effects models in lme4, such as fitting models with more complex random effects structures. Bayesian models also permit estimation of individual effect sizes and, critically, an index of uncertainty from a group-level model. While an in-depth tutorial on Bayesian statistics is outside the scope of this work, we summarize the main principles here. Briefly, Bayesian data analysis is based on the idea that we can estimate a probability distribution for an effect (the posterior distribution) from the information contained in the data (the likelihood) and our prior knowledge of the effect [the prior, @nalborczykIntroductionBayesianMultilevel2019]. We can summarize an effect by calculating the mean or median of the posterior distribution and represent an effect's uncertainty by describing the tails of the posterior distribution (i.e., a Bayesian credible interval). For example, a 95% credible interval is interpreted such that there is a 95% probability that the interval contains the true effect, given the data and prior assumptions. Bayesian models can improve model estimation for small sample sizes and often permit greater complexity in the random effect structures. This is important because type-1 errors may increase in models with a reduced random effect structure, which is a common practice given that convergence issues may arise with complex models, especially in small data sets. In the following, we demonstrate how researchers may use the brms package [Bayesian Regression Models using Stan, @burknerBrmsPackageBayesian2017] to implement the group model. To implement the group model, we subset the data for only the treated items in the blocked condition.

```{r, message = FALSE, warning = FALSE, cache = TRUE}
df_glmm <- df %>%
  # select the correct phase, condition, and itemType
  filter(phase == "baseline" | phase == "treatment",
         condition == "blocked",
         itemType == "tx") %>%
  # create the Huitema model parameters
  mutate(baseline_slope = session,
         level_change = ifelse(phase == "baseline", 0, 1),
         slope_change = (baseline_slope - (n_baselines+2))*level_change,
         level_change = as.factor(level_change))
```

To fit the three-level model for multiple participants in the brms R package [@burknerBrmsPackageBayesian2017], only a few modifications are required of the lme4 code. First, the function changes from `glmer()` to `brm()`. Second, a group-level intercept for participant and slopes for baseline slope, level change, and slope change are added to permit participants to vary in their trend during baseline, initial level change, and overall slope change. Group-level intercepts are also included for items. Third, the `brm()` function family argument specifies the Bernoulli family as the special case of the binomial family, where each observation represents a single trial. Additionally, users should specify the total number of iterations, the number of iterations to remove at the beginning of sampling, the number of Markov chains, a seed for reproducibility, and importantly, prior distributions. In this case, we have included a prior distribution characterized by a mean of zero and standard deviation of 2.5 logits for the baseline slope, level change, and slope change effects, indicating that we anticipate that these effects are highly likely to fall within -5 and +5 logits (two standard deviations). At most, this prior distribution would correspond to improvements of about 85 percentage points for a participant starting around 7.5% accuracy. This prior is based on our previous use of this model structure @evansPlayingBEARSBalancing2021 and our understanding of what range of values constitute reasonable effect sizes. Similarly, we have included a prior on the intercept, with a mean of -1 and a standard deviation of 2.5. This prior expresses our knowledge of stimulus selection procedures in the study and the expectation that performance at the start of baseline will be poor (\~27%) but with a wide range of plausible values (\~0% accuracy to 72% accuracy). These priors help to past experience and incorporate domain knowledge into the modeling approach by establishing beforehand knowledge of the data, including the range of reasonable values for the variables in the model.

```{r, warning = FALSE, message = FALSE, cache = TRUE}
mod3 <-
 brm(
  # population level effects (similar to fixed effects)
  response ~ 0 + Intercept + baseline_slope + level_change + slope_change +
   # group level effects (similar to random effects)
   # by-participant group-level effects
  (1 + baseline_slope + level_change + slope_change | participant) +
   # by-item group-level effects 
  (1|item),
  data = df_glmm, # the data used for the model
  family = bernoulli(), # special case of binomial with n=1 trials
  iter = 3000, # number of draws per chain
  warmup = 1000, # number of draws to toss on "burn in"
  chains = 4, # total number of chains
  seed = 42, # set a seed
  prior = c( # prior distributions
       prior(normal(-1, 2), class = b, coef = Intercept),
       prior(normal(0, 2.5), class = b)),
  # extra arguments, see rmd file
  cores = 4,
  file = here("models", "group_brm"),
  file_refit = "on_change")
```

After checking that the model demonstrates adequate fit and convergence (see supplemental materials S1), we can examine the model using `summary()`. Again, printed are the population-level effects (analogous to fixed effects), which are notably very similar to the frequentist three-level model. The model summary also returns a 95% credible interval, a convergence statistic ($\hat{R}$), and the effective sample size, an estimate of the number of independent Markov chain samples absent of autocorrelation (not pictured; the reader is referred to @nalborczykIntroductionBayesianMultilevel2019 and supplemental materials S1. for interpretation guidelines).

```{r, warning = FALSE, message = FALSE, cache = TRUE}
# remove $fixed[,0:4] to see the entire summary
summary(mod3)$fixed[,0:4]
```

Individual effect sizes can be obtained by contrasting the model's posterior predictions for each participant between the end of baseline and onset of the treatment phase in logits, odds-ratios, percentage gain, or items gained. This process is conceptually similar to the approaches described for the individual-level mixed-effects models, and well-commented code is available in the supplemental materials S2 (omitted here due to length). The result is a summary of an estimated distribution of effect sizes in logits for each participant, with the median of the distribution as the effect size (column ES) and a 95% credible interval (.lower and .upper). Effect sizes generated in this manner can be converted to odds ratios, percentage point gain, or an estimate of the number of items gained.

<!-- This is taken from S2 -->

```{r}
# Printing the top 6 rows of the effect size table reported in S2.
head(bayesian_es, n = 6)
```

Researchers can interpret the magnitude of these effect sizes and examine how much of participants' credible intervals exceeds 0. Researchers can also define a range of values that are large enough to be clinically meaningful, and compare each individual's effect size distribution to this range in order to examine how many participants demonstrated clinically meaningful effects [@kruschke2018bayesian].

# Considerations for selecting effect sizes in small-N designs

In an ideal world, effect sizes in small-N studies are sensitive to a wide range of response patterns, provide a measure of confidence around the estimate of treatment response, and are interpretable by clinicians and researchers. Most importantly, they should be robust to experimental manipulation so that potential differences in effect sizes across cases and studies inform treatment theory and allow for conceptual replication. As Pustejovsky [-@pustejovsky2019procedural] writes, "An effect size that is instead sensitive to such procedural features can appear to be larger (or smaller) because of how the study was conducted rather than because treatment actually produced large (or small) effects" [@pustejovsky2019procedural , p. 218].

For example, consider recent work examining the effects of semantically-oriented anomia treatments for aphasia. Different research groups have provided converging and divergent findings for the efficacy and generalization of semantic treatments over the past three decades [e.g., @boyleSemanticFeatureAnalysis2010; @evansPlayingBEARSBalancing2021; @gilmoreTypicalitybasedSemanticTreatment2020] using a variety of related treatment approaches, but also differences in study designs and analytical methods. In order to make strong conclusions about conceptual replication or non-replication between these studies, we need to understand how methodological design decisions and analytical approaches affect conclusions about treatment response. In the following, we review the strengths and weaknesses of the different effect size measures to help clinician-researchers, and clinicians, make informed choices of effect sizes and act as informed consumers when interpreting related findings between research groups and studies.

There are substantial differences in the mathematical and conceptual approaches to the methods used to calculate effect sizes described above. Each effect size permits different conclusions about the data, may map to different formulations of the research question, and is differentially sensitive to methodological decisions in small-N designs. To illustrate these differences, we calculated effect sizes for all series in Wambaugh et al., (2017).

Following Wambaugh et al., (2017), $d_{BR}$ was calculated, which compared the mean performance for the five baseline timepoints preceding the onset of the intervention to the mean of the last two timepoints of the treatment phase. We omitted series where baseline variance was zero, choosing not to pool the baseline variance or derive it from a separate series. Proportion of Potential Maximal Gain was calculated using the same observations as $d_{BR}$. Tau-U was calculated following the methods of Lee and Cherney (2018), including all observations in the baseline and treatment phases. $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$ was used when a linear trend line in the baseline phase exceeded 0.33, otherwise $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$ was calculated. For simplicity, we collapsed performance across phonemes for $d_{BR}$, PMG, and Tau-U.

Because of a large number of convergence errors, even with overly simplified random-effects structures, while running individual-level models using the R package lme4 [@batesFittingLinearMixedEffects2015a], we estimated mixed-effects model-based individual effect sizes using three-level Bayesian mixed-effect models (two models for each item type and two models for each condition, 4 total models). Based on our experience, this approach generally returns individual effect size estimates similar to those produced when frequentist models converge. Individual effect sizes were estimated based on the model predictions of performance differences between the end of the treatment phase and the end of the baseline phase in terms of logits and percentage point change. Priors and fitting procedures are reported in the S2.

We provide an interactive web app to allow each effect size measure to be compared along with each participant's performance: <https://rb-cavanaugh.shinyapps.io/reproducibile-small-n>. Effect sizes in the web app can be adjusted by modifying several researcher degrees of freedom, such as the choice to include all baseline data, set the Tau-U cutoff at 0.33 or 0.4, or extrapolate the baseline slope for the GLMM effect sizes. Throughout the remainder of the paper, we refer the reader to specific examples from the participants in Wambaugh et al., (2017) where choice of effect size or analytical decisions impact interpretation of the effect of the intervetion. These examples can be easily visualized using the web-app. Readers can also use it to form a stronger intuitive understanding of the relationships between different measures and performance using real data. Additionally, the relationships between effect sizes are shown in Figure 3, a scatterplot matrix between the effect size measures that are the focus of this paper. This figure is available in the web-app, and will change as a result of chosen analytical decisions. Broadly speaking the scatterplots and correlations demonstrate substantial differences between the various effect size approaches (Figure 3). We discuss the reasons for these differences below.

##### Figure 3. About here

```{r, include = FALSE}
# Generated in supplemental materials S2. 
knitr::include_graphics(here("manuscript", "tables-figures", "p3.png"))
```

## Within-case standardized mean difference ($d_{BR}$)

The primary feature of the scatterplots comparing $d_{BR}$ with other effect size measures (first column) is the heteroscedasticity, that is, the variability in the relationship between $d_{BR}$ and the other effect size measures increases as $d_{BR}$ increases in size. This heteroscedasticity can be explained by considering that large values of $d_{BR}$ can occur due to large changes in performance, low baseline performance variability, or both. No other effect size measure uses baseline variability to index change (though notably, the mixed-effects models use it to estimate effect size uncertainty). While the initial motivation for standardizing within-case change by the baseline variability was to create a "standardized" metric that allowed for meta-analysis and comparison across studies (Gingerich, 1984, p. 75), the within-case standardized mean difference is sensitive to study design choices that should not affect an effect size (e.g., the number of baseline probe sessions, Pustejovsky, 2019), which makes comparison between studies, or even participants within the same study, tenuous at best. In aphasia and related disorders, the $d_{BR}$ statistic is also typically compared to a meta-analytic "benchmark" study [e.g., @bailey2015sound; @beesonEvaluatingSinglesubjectTreatment2006]. However, because the $d_{BR}$ statistic is sensitive to differences in study designs, the benefit of such comparisons is likely limited at best.

Further complicating comparisons across studies or to meta-analytic benchmarks is the fact that the calculation of $d_{BR}$ often varies between studies. Authors may choose to include some or all baseline or treatment observations, accommodate near-zero baseline variability by substituting baseline variance from other conditions or participants, or average two $d_{BR}$ scores calculated within-list versus calculating a single $d_{BR}$ statistic for each list. Wambaugh et al., (2017) calcuated $d_{BR}$ for each phoneme within each list, consistent with the SPT benchmark study (Bailey et al., 2015). However, calculating $d_{BR}$ collapsing across the two phonemes (as is often done with semantic category in semantically-focused treatments) would have resulted in a substantially larger average $d_{BR}$ effect sizes for the treated (13.5 vs. 8.4) and generalization (5.1 vs. 2.9) items. Participant 8 (treated, random condition) provides a clear example of the effect of averaging $d_{BR}$ across two conditions versus collapsing performance before calculating $d_{BR}$. On the other hand, Wambaugh et al., (2017) used the last five baseline observations preceding the onset of the intervention rather than all baseline time points regardless of phase length (Bailey et al., 2015). Including all versus the last five baseline time points can influence the mean of baseline performance and baseline variability, and thus impact $d_{BR}$. For example, including only the last five baseline observations nearly doubles $d_{BR}$ for participant 9 (treated, random condition), even though the average baseline performance during the last 5 observations is higher than the mean of the entire baseline phase. These differences limit direct comparison of $d_{BR}$ across studies, and their effects are rarely discussed even when they are reported. To make $d_{BR}$ comparable across studies and reproducible, researchers must minimally report the absolute change, baseline variability, and any deviations from established benchmark studies.

There are additional outstanding criticisms of $d_{BR}$ that may negatively impact its utility, which have been described previously: (1) assumptions of independent observations and constant variance required by $d_{BR}$ are rarely met in single-case subject designs [@howardOptimisingDesignIntervention2015]; (2) benchmarks for interpreting $d_{BR}$ must be established before $d_{BR}$ can be interpreted; (3) $d_{BR}$ is often applied to binomially distributed data, where the mean and standard deviation are related, thus introducing bias dependent on the level of baseline performance. (4) $d_{BR}$ is influenced by autocorrelation [@archerEffectSizesSingleCase2019], where performance at one probe session is correlated with the previous session. (5) $d_{BR}$ does not account for baseline trends. We have included $d_{BR}$ in this tutorial given its current widespread use. However, given these considerable limitations and methodological complications, we do not recommend its continued use in future studies.

## Proportion of Potential Maximal Gain

A notable characteristic of PMG in Figure 3 is the similarity between PMG and the mixed-effect model effect sizes, particularly in terms of percent gain. This relationship is expected for studies such as Wambaugh et al., (2017), which use strict stimulus-selection methods that are matched to participant ability. When baseline performance is similar across individuals, PMG will be highly correlated with absolute change (e.g., percentage point gain estimated by the mixed-effects models). The downside of this approach is that where absolute change is equivalent, differences in PMG are attributable to differences in baseline performance. For example, if a group of participants has the same absolute change (an increase of 10 out of 20 items), PMG can vary drastically: from 0.5 if a participant averages 0% correct at baseline to 1.0 if a participant averages 50% correct at baseline. For example, examine participants 11 and 12 (treated items, blocked condition, using the last 5 baseline observations) in the web-app. Both participants improve by roughly 9 items, but PMG = 0.95 for participant 11 and PMG = 0.62 for participant 12. The difference is driven only because participant 11 averaged about 50% (10 items) correct at baseline while participant 12 averaged about 25% (5 items) correct. The consequence of this feature is that for studies that use stringent stimulus selection procedures, PMG will largely index absolute change. Alternatively, for studies with more variability in baseline performance across participants (e.g., studies which providing the same items to all participants), PMG will be correlated with baseline severity even if severity does not moderate response to treatment. For such studies, finding that treatment was more beneficial for milder participants could simply be an artifact of the choice of PMG as an effect size. Ultimately, for appropriate comparison of performance between participants and across studies, researchers must consider these features of PMG.

PMG was intended to be used in analyses where participants received a different number of treated items or to account for baseline severity when the same items were assigned to all participants. PMG serves a similar purpose in this data, providing a measure of change on the same scale for all participants, even though 4 participants received fewer treated and untreated items in a modified protocol (see Wambaugh et al., 2016). As with $d_{BR}$, PMG does not provide a level of certainty; thus, estimating PMG cannot distinguish whether or not change is unlikely to occur by chance alone, or whether two values of PMG might be different. Ultimately, alternative methods of estimating individual effect sizes can better account for differences in item set size (e.g., mixed-effects models), have less potential for dependence on baseline severity, and include a measure of uncertainty. For these reasons, we recommend researchers pursue other effect size measures if applicable to their study design, particularly if they are interested in the relationship between aphasia severity and treatment response. If used, best practice would include reproducible analyses that report unstandardized change scores.

## Tau-U

Tau-U is a non-parametric effect size measure designed to demonstrate the degree of overlap between phases, but is not intended to distinguish between the magnitude of treatment response when there is no overlap. The large degree to which Tau-U diverges from other effect sizes is readily apparent in Figure 3, where there are clear ceiling effects. In other words, where Tau-U is equal to 1, there is a wide variation in the other effect size measures. These cases are characterized by no overlap between the baseline and treatment phases but widely varying degrees of change from baseline to the end of treatment. For example, compare participant 1 and participant 2 (treated items, blocked condition). Both participants have Tau-U scores = 1, but participant 1's absolute change is less than half of participant 2. This limitation of Tau-U has been discussed previously [@woleryComparisonOverlapMethods2010] but is often overlooked. The consequence is that interpretations of Tau-U [e.g., large: 0.60 to 0.80, very large: 0.80 to 1, @vannestEvaluatingInterventionEffects2015] are not comparable to other effect sizes discussed here. For example, Tau-U fell in the "large" or "very large" range for nearly all treated conditions, but ranged from no effect to a large effect size by $d_{BR}$ standards. Researchers, reviewers, and consumers should be aware of this conceptual difference when interpreting Tau-U and comparing effect sizes across the literature.

There have been a number of additional criticisms of Tau-U recently, summarized by Tarlow [-@tarlowImprovedRankCorrelation2017]: (1) Tau-U has inconsistent terminology and multiple mathematical definitions, which generate different values and thus require researchers to be aware of and transparent about which Tau-U they have employed, (2) While $\mathrm{Tau\textrm{-}U_{A\:VS.\:B}}$ is bound between -1 and 1, the baseline-corrected $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$ is not, and can return inflated values ranging from -2 to 2 (3) Tau-U data cannot be visualized graphically (4) the degree of baseline correction is influenced by the ratio of the number of observations in the baseline phase to the treatment phase. Like $d_{BR}$, this final point is pertinent to studies such as Wambaugh et al., 2017, where the number of baseline points varies across participants to demonstrate experimental control and also varies based on whether an intervention was provided first or second within an individual participant. Moreover, the choice of cutoff for using a baseline correction can affect Tau-U estimates between participants with varying degrees of baseline trend within and across studies. Finally, there is no clear confidence interval for $\mathrm{Tau\textrm{-}U_{A\:VS.\:B\:-\:TREND\:A}}$ [@pustejovsky2018singlecasees], which is evident in using the `Tau()` versus `Tau_U()` functions in the SingleCaseES package.

## Generalized Linear Mixed-effects models

While frequentist mixed-effects models address some of the limitations of $d_{BR}$, PMG, and Tau-U, the difficulty implementing individual-level models with the present data is typical of challenges using item-level generalized linear mixed-effects models in our experience. Wiley and Rapp (2018) suggest that individual models can be run to statistically examine change and estimate effect sizes for each participant. However, a substantial number of individual-level models in the Wambaugh et al., (2017) dataset failed to converge, even with overly-simplified random effects structures, which are likely to return anti-conservative standard errors for repeated measures data. The reason for non-convergence and singular fit warnings in this data likely stem from two issues: relatively few items per list (especially in the case of generalization items) and some occasions of near-complete separation (i.e., performance at floor) during the baseline phase. The best practices for dealing with convergence and fit warnings are still a matter of debate [@meteyardBestPracticeGuidance2020]. How researchers accommodate non-convergence and singular fit introduces additional "researcher degrees of freedom," which can impact replication. These decisions add to the complexity of mixed-effect models and underscore the need for reproducible analysis when they are used.

One benefit of the interrupted time series model is the ability to adjust for a baseline trend, though determining the cases in which to do so can be challenging [@manolov2019extrapolating]. Calculating effect sizes using the more conservative method of projecting out performance to the end of treatment based on baseline trend may underestimate change if a baseline trend levels off and stabilizes before the start of an intervention. Alternatively, failing to account for a trend may attribute too much change to the onset of treatment (see participant 20, treated items, random condition). Visual or statistical analysis of the baseline trend may be useful, nothing that if there are less than 5 baseline measurement occasions, the trend may be unreliable [@Huitema2011-dv]. Overall baseline trends may be poor predictors of ongoing performance if stability is reached in the final 3-5 baseline sessions; it may not be appropriate to extrapolate the baseline slope in these cases (see participant 16, treated items, random condition). The use of an unrelated "untreated" condition that is simply exposed to repeated probing through the intervention may further clarify whether or not it is necessary to extrapolate the baseline phase. If there is minimal change due to repeated probing in a balanced, untreated condition, a reasonable assumption may be that baseline trends reflect noise rather than a trend.

Another benefit specific to the generalized linear mixed-effects approach is the ability to report effect sizes in different units: logits, odds-ratios, percentage point gain, or the number of items gained, which can inform different research questions. Effect sizes in units of logits and odds-ratios are unbounded and insensitive to item set size and place more value on change for individuals who perform closer to floor or ceiling at baseline. For example, logits and odds ratios will indicate that a 5-item change from 0/20 to 5/20 items or 15/20 to 20/20 items is greater than a 5-item change from 8/20 to 13/20 items. Percentage point gain is similarly agnostic to item set size, but will return the same effect size for the three cases noted above (a 25 percentage point improvement). Interpreting change in terms of the number of items gained will reflect differences in item set size, and similarly considers these three cases to be equivalent (a 5 item improvement). The non-linear relationships between these scales make them helpful for answering different research questions. For example, if a researcher is interested in comparing two studies that treat substantially different numbers of items, it may be desirable to use the number of items gained since the other units may overstate relative improvement in the study with fewer items. Additionally, when baseline performance is highly variable across participants, using logits may be advantageous because they are less subject to floor and ceiling effects whereas percentage point gain or the number of items gained may unfairly penalize participants who perform well during the baseline phase, as they have less room for improvement.

Bayesian models, for both individual participants and groups, are often well-suited to smaller sample sizes and recommended for convergence and fit challenges in frequentist mixed-effects models [@batesFittingLinearMixedEffects2015a]. Bayesian models for groups of participants readily provide estimates of effect sizes and associated credible intervals for individuals, which allows researchers not only to establish whether a treatment worked on average, but also to identify the number of participants for whom there is reliable evidence of a treatment effect - a critical need for future aphasia research [@breitenstein2022operationalising]. Estimating individual-level effects from a group model can improve reliability and reduce overfitting through partial pooling, where extreme observations are pulled towards the group average [@nalborczykIntroductionBayesianMultilevel2019].

One criticism of Bayesian statistics concerns the use of priors, which some researchers argue can have an outsized influence on model results. In this tutorial, and papers used in our lab, we generally advocate for using "weakly regularizing" prior distributions, which improve model sampling and estimation, only assuming a range of plausible treatment effects centered around zero. When used appropriately, this feature of the Bayesian approach helps to constrain findings to what may be considered an a priori reasonable range of effects, reduce regression to the mean, and formally incorporate researchers' existing knowledge to increase the precision of model parameter estimates [@nalborczykIntroductionBayesianMultilevel2019].

Neither the frequentist nor Bayesian implementations of the interrupted time series model (Huitema & Mckean, 2000) demonstrated discussed in this tutorial explicitly account for temporal autocorrelation (e.g., the correlation between performance on adjacent sessions), though it may be advantageous to do so in future work. Additionally, the ability to detect reliable changes is dependent on sample size, in terms of the number of participants, items, and measurement occasions. Simulation-based power analysis may be used to anticipate the needed sample sizes and the growing number of studies using similar models [@evansPlayingBEARSBalancing2021; @swiderski2021treatment], and the present data, should make power analysis for these models more feasible.

# General Discussion

In this tutorial, we demonstrated how to conduct reproducible analysis of small-N treatment research using the statistical programming language R and how effect size selection and implementation can affect the interpretation and replication of findings within and across studies. This tutorial aims to serve as a starting place for researchers engaged in small-N studies to begin incorporating reproducible analyses into their regular workflow and better understand how their choice of effect size measure can threaten successful replication. Discussions of effect size strengths and weaknesses are intended to help researchers and clinicians be more informed consumers of this important body of research.

While we have described a number of differences across analytical approaches, all methods described here require researchers to make many small decisions in the process of analyzing small-N data. Which data points do you include in your analysis? Should you pool variance across phases, and how do you adjust $d_{BR}$ for cases of no baseline variability? How much of a baseline trend should be corrected for Tau-U? How do you address mixed-effects model non-convergence? How do you specify your random effects? Many of these decisions are difficult to anticipate a-priori during study preregistration, yet they are critical for promoting successful study replication. The extent and influence of these degrees of freedom underlie the importance of reporting fully reproducible analyses, promoting transparency in small-N design research. Such dissemination also reduces barriers for successful meta-analysis of small-N designs, fundamental to arriving at scientific consensus.

While this tutorial has focused on effect sizes common to the small-N literature in aphasia and related disorders, there are novel effect sizes with desirable qualities that are likely to be of interest to the field. For example, the gradual effects model [@swan2018gradual] is able to capture non-linear change under normal, count, and binomial data generating processes, and describe change in an interpretable, unstandardized effect size. The Log-response and Log-odds ratios also have desirable properties for estimating change across the binomial and count data typical of small-N studies in aphasia [@pustejovsky2018using].

We offer modest guidance for researchers wondering about the "best" analytical approach and effect sizes for small-N designs in aphasia and related disorders, based on our discussion and comparison of effect sizes in Wambaugh et al., (2017). First and foremost, researchers should select effect sizes that align with their specific research questions and are well-suited to their study design. Any discontinuity between the research question and statistical method limits the conclusion drawn from the study. Second, while all effect sizes reviewed in this tutorial have limitations, researchers should be particularly cautious in their use of $d_{BR}$, PMG, given the lack of established confidence intervals and sensitivity to experimental manipulations. The Tau-U statistics are well supported in the broader single-case experimental design literature, but only describe the degree of non-overlap between phases rather than the magnitude of treatment response.

Of the effect sizes common to small-N studies in aphasia and related disorders, we recommend using mixed-effects models, which can generate effect sizes that are accompanied by uncertainty and are more robust to experimental manipulations. While this tutorial was intended to make mixed-effects models more approachable, we recognize that they are complex and require additional statistical expertise. The novel approaches noted (i.e., the gradual effects model, log-response and log-odds ratios) above may strike a better compromise between complexity and rigor but are outside of the scope of this tutorial. Ultimately, the reality is that choosing the "best" effect size is highly context-dependent. Researchers must be knowledgeable about the strengths and weaknesses of their chosen method and transparent about how their methodological decisions and the choice of statistical method might influence their conclusions.

Pursuing reproducible research using script-based approaches is a critical first step in addressing the challenges common to analyzing small-N studies in aphasia and related disorders. Given the impact of small-N studies on clinical rehabilitation services, we argue that sharing data and script-based analyses is research best practice and should be a minimum standard for our field. Pairing reproducible analyses with informed selection of effect sizes can improve scientific rigor and transparency and the mapping between research questions and analytical techniques and facilitate more robust tests of conceptual replications across studies.

# Acknowledgements

Research reported in this manuscript was supported by the National Institute of Deafness and Communication Disorders of the National Institutes of Health under award number F31DC019853-01 awarded to Robert Cavanaugh. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The contents of this manuscript were also developed under a grant from the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR) awarded to Yina Quique. NIDILRR is a Center within the Administration for Community Living (ACL), Department of Health and Human Services (HHS). The contents of this manuscript do not necessarily represent the policy of NIDILRR, ACL, or HHS, and you should not assume endorsement by the Federal Government.

# References

::: {#refs}
:::

::: {custom-style="noIndentParagraph"}
# Table and Figure Captions

Figure 1. Participant 10 performance during baseline and treatment phase for the blocked condition. Dark circles indicate data points used to calculate dBR and PMG.

Figure 2. Participant 10 performance during baseline and treatment phase from Wambaugh et al., (2017). Plot annotations indicate Huitema & McKean (2000) model coefficients.

Figure 3. Relationships between individual effect size measures typically used in aphasia small-N studies.

Table 1. Variables and descriptions for study data from Wambaugh et al., (2017)

Table 2. The first 5 rows of data
:::
